{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53tnV3Ar7VzI",
        "outputId": "cb73b25c-648e-4013-cd0c-fa0f16f8ac03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.4MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 162kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.09MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ANN...\n",
            "Epoch 1/5: Train Loss: 0.629, Acc: 77.1% | Val Loss: 0.428, Acc: 83.9%\n",
            "Epoch 2/5: Train Loss: 0.451, Acc: 83.9% | Val Loss: 0.381, Acc: 86.2%\n",
            "Epoch 3/5: Train Loss: 0.413, Acc: 85.1% | Val Loss: 0.373, Acc: 86.3%\n",
            "Epoch 4/5: Train Loss: 0.388, Acc: 86.1% | Val Loss: 0.353, Acc: 87.3%\n",
            "Epoch 5/5: Train Loss: 0.370, Acc: 86.7% | Val Loss: 0.342, Acc: 87.7%\n",
            "Training CNN...\n",
            "Epoch 1/5: Train Loss: 0.557, Acc: 79.6% | Val Loss: 0.367, Acc: 86.3%\n",
            "Epoch 2/5: Train Loss: 0.371, Acc: 86.6% | Val Loss: 0.288, Acc: 89.2%\n",
            "Epoch 3/5: Train Loss: 0.319, Acc: 88.3% | Val Loss: 0.257, Acc: 90.3%\n",
            "Epoch 4/5: Train Loss: 0.288, Acc: 89.4% | Val Loss: 0.241, Acc: 90.7%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# 1. Load the Dataset and Preprocessing\n",
        "transform_ann = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.RandomRotation(10)  # Augmentation for CNN\n",
        "])\n",
        "\n",
        "# Load full training dataset\n",
        "full_train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_ann\n",
        ")\n",
        "\n",
        "# Split into train and validation (80/20)\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Apply different transforms to subsets if needed; here reusing for simplicity\n",
        "# For CNN, we'll apply augmentation only to train_dataset during loading\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=False, download=True, transform=transform_ann\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_loader_ann = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# For CNN, create separate train_loader with augmentation\n",
        "train_dataset_cnn = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=True, download=True, transform=transform_cnn\n",
        ")\n",
        "train_dataset_cnn, _ = random_split(train_dataset_cnn, [train_size, val_size])  # Reuse split\n",
        "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Class names\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# 2. Model Definitions\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 3. Training Function\n",
        "def train_model(model, train_loader, val_loader, epochs=5, lr=0.001):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss_avg = val_loss / len(val_loader)\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "        val_losses.append(val_loss_avg)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.3f}, Acc: {train_acc:.1f}% | Val Loss: {val_loss_avg:.3f}, Acc: {val_acc:.1f}%')\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "# 4. Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_acc = 100 * sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return test_acc, cm\n",
        "\n",
        "# Train ANN\n",
        "print(\"Training ANN...\")\n",
        "ann_model = ANN()\n",
        "ann_train_losses, ann_val_losses, ann_train_accs, ann_val_accs = train_model(ann_model, train_loader_ann, val_loader)\n",
        "\n",
        "# Train CNN\n",
        "print(\"Training CNN...\")\n",
        "cnn_model = CNN()\n",
        "cnn_train_losses, cnn_val_losses, cnn_train_accs, cnn_val_accs = train_model(cnn_model, train_loader_cnn, val_loader)\n",
        "\n",
        "# Evaluate\n",
        "ann_test_acc, ann_cm = evaluate_model(ann_model, test_loader)\n",
        "cnn_test_acc, cnn_cm = evaluate_model(cnn_model, test_loader)\n",
        "\n",
        "print(f'ANN Test Accuracy: {ann_test_acc:.1f}%')\n",
        "print(f'CNN Test Accuracy: {cnn_test_acc:.1f}%')\n",
        "\n",
        "# Plot Accuracy Curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ann_train_accs, label='ANN Train')\n",
        "plt.plot(ann_val_accs, label='ANN Val')\n",
        "plt.plot(cnn_train_accs, label='CNN Train')\n",
        "plt.plot(cnn_val_accs, label='CNN Val')\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Loss Curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ann_train_losses, label='ANN Train Loss')\n",
        "plt.plot(ann_val_losses, label='ANN Val Loss')\n",
        "plt.plot(cnn_train_losses, label='CNN Train Loss')\n",
        "plt.plot(cnn_val_losses, label='CNN Val Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for ANN (example)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(ann_cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.title('ANN Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for CNN\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cnn_cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.title('CNN Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Example predictions (optional)\n",
        "def show_predictions(model, test_loader, num_images=5):\n",
        "    model.eval()\n",
        "    dataiter = iter(test_loader)\n",
        "    images, labels = next(dataiter)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "    for idx in range(num_images):\n",
        "        ax = fig.add_subplot(1, num_images, idx+1)\n",
        "        img = images[idx].cpu().squeeze()\n",
        "        img = (img * 0.5) + 0.5  # Denormalize\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_title(f'True: {classes[labels[idx]]}\\nPred: {classes[predicted[idx]]}')\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "print(\"ANN Predictions:\")\n",
        "show_predictions(ann_model, test_loader)\n",
        "print(\"CNN Predictions:\")\n",
        "show_predictions(cnn_model, test_loader)"
      ]
    }
  ]
}